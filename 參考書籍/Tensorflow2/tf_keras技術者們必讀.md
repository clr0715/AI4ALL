#
```
tf.keras 技術者們必讀！深度學習攻略手冊
書籍類別：人工智慧/人工智慧
作者：施威銘研究室 著
書號：F0380

https://www.flag.com.tw/bk/t/f0380

第 0 章 行前準備：NumPy 陣列快速上手
0-0 陣列的 shape 與軸、階、維度
0-1 NumPy 陣列的屬性與型別轉換
0-2 建立 NumPy 陣列 (array)
0-3 陣列切片 (slicing)：截取陣列片段
0-4 陣列的重塑與轉置
0-5 陣列間對應元素 (element-wise) 的運算
0-6 陣列擴張 (broadcasting)
0-7 組合多個索引來批次設值或取值
0-8 沿著陣列的某一軸做運算

第 1 章 初探 Keras：建構第一隻神經網路
1-0 神經網路基礎
1-1 神經網路的學習方式
單一神經元的學習方式
整個神經網路的學習方式
1-2 Keras 及 tf.keras 速覽
多後端的Keras
Tensorflow的tf.keras
1-3 用 6 行程式建構神經網路
建立空的神經網路模型
加入第一層的神經層(兼具輸入層功能)
加入第二層的神經層(做為輸出層)
指定訓練及評量方式來編譯模型
1-4 訓練神經網路的流程 - 以辨識手寫數字圖片為例
0. 準備訓練所需的資料
1. 資料預處理(Preprocess)
2. 建立與編譯模型
3. 訓練模型
4. 評估模型成效及修正
5. 用模型預測答案
6. 模型的儲存及載入
1-5 實驗：以自製手寫數字圖片測試
動手製作3種手寫數字圖片
實驗1：測試原始圖片, 但不做前置影像調整
實驗2：做前置影像調整後再測試
實驗3：測試數字位置對準確率的影響
實驗4：測試數字大小對準確率的影響
實驗5：測試線條邊緣漸層對準確率的影響
實驗6：測試線條粗細對準確率的影響
實驗7：測試線條深淺對準確率的影響
實驗8：找出自製圖片的最佳調整方式
實驗9：用最佳調整方式重新訓練模型
1-6 NumPy 陣列的點積運算 (dot product)
向量的點積運算
矩陣的點積運算
多軸陣列的點積運算
1-7 密集神經網路的張量運算

第 2 章 序列模型與密集神經網路
2-0 再論序列模型
2-1 存取模型的結構或權重參數
2-2 模型的編譯：compile()
2-2-0 Compile()的loss參數
實驗：加大或縮小損失值對訓練成效的影響
2-2-1 Compile()的metrics參數
2-2-2 Compile()的optimizer參數
2-2-3 compile()全部參數的詳細用法
2-2-4 compile()可能發生的例外
2-3 模型的訓練：fit()
fit()的傳回值
fit()可能發生的例外
實驗：不同batch_size對訓練成效的影響
2-4 模型的評估：evaluate()
2-5 模型的預測：predict()
2-6 用一小批資料做訓練、評估、或預測
2-7 密集層 (Dense Layer)
2-8 實例：二元分類 - 使用 IMDB 做正負評分類
2-9 實例：多元分類 - 將路透社新聞分類成 46 個主題
2-10 實例：迴歸預測 - 使用美國研究所錄取資料集來預測錄取機率
實驗：使用不同的神經層數、神經元數、及批次量做測試
2-11 火力加強 – 使用生成器及 Sequence 物件

第 3 章 卷積神經網路 (CNN)
3-0 從密集神經網路到卷積神經網路
3-1 卷積層 (Convolutional Layer)
3-2 池化層 (Pooling Layer)
最大池化MaxPooling2D
平均池化AveragePooling2D
降維方式的抉擇
3-3 展平層 (Flatten)
3-4 密集層 (Dense Layer)
3-5 丟棄層 (Dropout)
3-6 實戰：訓練 CNN 辨識 CIFAR-10 圖片資料集

第 4 章 循環神經網路 (RNN)
4-0 以 SimpleRNN 找到資料中的週期
4-1 時序資料預處理 Sequence Preprocessing
4-2 Stateful RNN
實例：Dense VS RNN VS Stateful RNN
4-3 文件資料預處理 Text Preprocessing
文字轉序列
Token 法
雜湊法
序列對齊
文字轉 one-hot
文字轉 multi-hot
文字轉詞向量
4-4 以嵌入層(Embedding layer)實作 IMDB 分類效果
4-5 使用循環丟棄法避免過度配適問題
4-6 堆疊循環層
4-7 長短期記憶 (LSTM) 和閘控循環單元 (GRU)
4-8 雙向循環層
4-9 結果與討論

第 5 章 函數式 API
5-0 函數式 API (Functional API) 快速上手
函數式 API 的建模方式
Input() 及 Model() 的參數設定
5-1 多輸入模型
5-1-0 可以合併多個分支的合併層
5-1-1 實例：建立有 3 個輸入層的模型
5-1-2 用程式產生訓練樣本及答案
5-1-3 訓練有 3 個輸入層的模型
5-1-4 利用 EMA 找出訓練成效最好的週期
5-1-5 重新訓練到最佳週期並評估成效
5-1-6 實驗：如果改用 2 種樣本資料來訓練模型呢？
5-2 多輸出模型
5-2-0 實例：將前面範例多加一個「評價」輸出層
5-2-1 產生「評價」標籤資料, 並依評價修改銷量
5-2-2 編譯多輸出模型
5-2-3 自訂評量函式
5-2-4 訓練多輸出模型
5-2-5 找出並訓練到最佳週期, 然後產生測試資料評估成效
5-2-6 實驗：增加樣本數量以提升準確率
5-3 函數式 API 的更多應用
5-3-0 內部分岔的有向無循環模型
5-3-1 層的共用與權重共享
5-3-2 層內的節點 (node)
5-3-3 實例：判斷 2 張手寫數字圖片是否為同一個數字
5-3-4 將模型做為層來使用
5-3-5 實例：在新模型中套用已訓練好的 CNN 模型
5-4 繪製模型的結構圖
5-5 實例：用「故事與問題」訓練雙輸入的 RNN 問答模型
5-5-0 資料集說明：The (20) QA bAbI tasks 資料集
5-5-1 下載 QA 資料集
5-5-1 撰寫解析檔案內容的 get_sqa() 函式
5-5-2 直接讀取壓縮檔的內容並轉換為 QA 資料集
5-5-3 資料預處理
5-5-4 建立及編譯雙輸入模型
5-5-5 訓練模型並評估成效
5-5-6 實驗 0：測試所有適用的 QA 任務
5-5-7 實驗 1：在故事樣本中加入無關 (非支持答案) 的敘述

第 6 章 預先訓練自己的中文詞向量
6-0 為什麼要預先訓練詞向量
6-1 Word2vec 實作原理
CBOW 連續詞袋模型
Skip-gram 跳字模型
進階 Skip-gram 模型
6-2 建立並訓練 Word2vec 神經網路
6-2-0 建立完整的 Word2vec 架構
6-2-1 取得原始資料 (語料)
6-2-2 資料預處理的介紹
6-2-3 資料預處理：產生訓練 Word2vec 所需的資料集
6-2-4 訓練 Word2vec
6-3 訓練新的神經網路進行中文文本分類
6-3-0 製作資料集：取得回答資料與話題種類
6-3-1 將資料集整理成神經網路可以接收的形式
6-3-2 訓練神經網路對回答資料進行話題分類
6-4 實驗
6-4-0 實驗 0：詞向量的差異
6-4-1 實驗 1：訓練資料筆數
6-4-2 實驗 2：訓練資料的字詞數量
6-4-3 實驗 3：另一種訓練資料不足的可能
6-4-4 實驗 4：多話題的分類
6-4-5 實驗 5：神經網路結構測試

第 7 章 進階應用
7-0 用 Callback 監控訓練過程
7-0-0 EarlyStopping
7-0-1 ModelCheckpoint
7-0-2 ReduceLROnPlateau
7-0-3 LearningRateScheduler
7-0-4 CSVLogger
7-0-5 撰寫你自己的 Callback
7-1 用 TensorBoard 解析訓練過程的歷史記錄
7-1-0 基本使用方式
7-1-1 tf.TensorBoard Callback 的其他參數

7-2 圖檔及影像處理技巧
7-2-0 檢視本節所附的樣本圖檔：貓狗資料集
7-2-1 可提供批次影像資料的 ImageDataGenerator
7-2-2 實例：用貓狗的照片檔來訓練、驗證、評估、預測模型
7-2-3 ImageDataGenerator 的資料擴增功能
7-2-4 實例：使用資料擴增提升訓練成效
7-2-5 實驗：進一步提升貓狗辨識的準確率
7-2-6 實驗：將資料擴增法及 Dropout 層應用在樣本不足的案例

7-3 各種 CNN 經典模型的建構祕方
7-3-0 LeNet
7-3-1 AlexNet
實驗：梯度消失 (Vanishing gradient)
7-3-2 VGG
7-3-3 Network in Network (NiN)
7-3-4 GoogLeNet：Inception-V1
7-3-5 Inception-V2、V3
實驗：在 40 層以上的神經網路中使用 BN 層
實驗：BN 層讓 Sigmoid 復活了
7-3-6 Inception-V4、Inception-ResNet
7-3-7 Xception
實驗：SeparableConv2D vs Conv2D
7-3-8 ResNet
7-3-9 DenseNet (Densely Connected Convolutional Networks)

7-4 遷移學習 - 以預訓練好的經典模型 VGG16 為例
7-4-0 什麼是遷移學習 (transfer learning)
7-4-1 萃取出資料的特徵
7-4-2 將經典 CNN 移植到新模型之中
7-4-3 模型的微調 (fine-tuning)

附錄 A 使用 Google 的 Colab 雲端開發環境
A-0 Colab 快速上手
A-1 Colab 雲端虛擬主機的管理與設定
A-2 Colab 的目錄窗格與檔案管理
A-3 Colab 的偏好設定
```
